{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "mnist = pd.read_csv('train.csv')\n",
    "\n",
    "train_labels = mnist['label']\n",
    "\n",
    "train_data = mnist.drop('label', 1)\n",
    "\n",
    "\n",
    "#trainnn = cv2.imread(\"images/image1.png\", 0).flatten()\n",
    "training_data =np.zeros((160000, 784))\n",
    "k=0\n",
    "\n",
    "#importing augmented image data as training data\n",
    "for i in range(32000):\n",
    "    str1 = \"images/image\" +str(i) + \".png\"\n",
    "    str2 = \"images/image\" +str(i) + \"__blur2.0.png\"\n",
    "    str3 = \"images/image\" +str(i) + \"__noise0.03.png\"\n",
    "    str4 = \"images/image\" +str(i) + \"__rot-30.png\"\n",
    "    str5 = \"images/image\" +str(i) + \"__trans3_3.png\"\n",
    "    \n",
    "    training_data[k] = cv2.imread(str1, 0).flatten()\n",
    "    k=k+1\n",
    "    training_data[k] = cv2.imread(str2, 0).flatten()\n",
    "    k=k+1\n",
    "    training_data[k] = cv2.imread(str3, 0).flatten()\n",
    "    k=k+1\n",
    "    training_data[k] = cv2.imread(str4, 0).flatten()\n",
    "    k=k+1\n",
    "    training_data[k] = cv2.imread(str5, 0).flatten()\n",
    "    k=k+1\n",
    "    \n",
    "    \n",
    "\n",
    "training_data = (training_data).astype('uint8')\n",
    "training_data = training_data/256\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_labels = np.zeros(160000)\n",
    "k =0 \n",
    "for i in range(32000):\n",
    "    training_labels[k] = train_labels[i]\n",
    "    k=k+1\n",
    "    training_labels[k] = train_labels[i]\n",
    "    k=k+1\n",
    "    training_labels[k] = train_labels[i]\n",
    "    k=k+1\n",
    "    training_labels[k] = train_labels[i]\n",
    "    k=k+1\n",
    "    training_labels[k] = train_labels[i]\n",
    "    k=k+1\n",
    "\n",
    "training_labels[154000]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "np.shape(train_data)\n",
    "\n",
    "labels_onehot = np.zeros((160000,10))\n",
    "        \n",
    "for i in range(160000): \n",
    "    for j in range(10):\n",
    "        if j == training_labels[i]:\n",
    "            labels_onehot[i,j] = 1\n",
    "        else:\n",
    "            labels_onehot[i,j] = 0\n",
    "\n",
    "\n",
    "\n",
    "def next_batch(data, labels_onehot, batch_size):\n",
    "        \n",
    "        \n",
    "        \n",
    "        A = random.randint(0, 160000 - batch_size-1)\n",
    "        B = A + batch_size\n",
    "        xs = data[A:B]\n",
    "        ys = labels_onehot[A:B]\n",
    "        \n",
    "    \n",
    "    \n",
    "        return xs, ys\n",
    "    \n",
    "        \n",
    "labels_onehot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "batch_size = 256\n",
    "\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "keep_prob= tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    #                        size of window         movement of window\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    weights = {'W_conv1':tf.Variable(tf.truncated_normal([5,5,1,32], stddev = 0.1)),\n",
    "               'W_conv2':tf.Variable(tf.truncated_normal([5,5,32,64], stddev = 0.1)),\n",
    "               'W_fc1':tf.Variable(tf.truncated_normal([7*7*64,1024], stddev = 0.1)),\n",
    "               'W_fc2':tf.Variable(tf.truncated_normal([1024,512], stddev = 0.1)),\n",
    "               'out':tf.Variable(tf.truncated_normal([512, n_classes], stddev = 0.1))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.constant(0.1, shape = [32])),\n",
    "               'b_conv2':tf.Variable(tf.constant(0.1, shape = [64])),\n",
    "               'b_fc1':tf.Variable(tf.constant(0.1, shape = [1024])),\n",
    "               'b_fc2':tf.Variable(tf.constant(0.1, shape = [512])),\n",
    "               'out':tf.Variable(tf.constant(0.1, shape = [n_classes]))}\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool2d(conv1)\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool2d(conv2)\n",
    "\n",
    "    fc = tf.reshape(conv2,[-1, 7*7*64])\n",
    "    \n",
    "    fc1 = tf.nn.relu(tf.matmul(fc, weights['W_fc1'])+biases['b_fc1'])\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    output = tf.matmul(fc2, weights['out'])+biases['out']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    sess= tf.Session()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(35000):\n",
    "        batch_xs, batch_ys = next_batch(training_data, labels_onehot, batch_size)\n",
    "        \n",
    "        sess.run(optimizer, feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 0.2})\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x:batch_xs, y: batch_ys, keep_prob: 0.5})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    \n",
    "    test = pd.read_csv('test.csv')\n",
    "    a = test.as_matrix()\n",
    "    a = (a/256)\n",
    "    \n",
    "    Imageid = np.linspace(1, 10000, num=10000, dtype = int)\n",
    "    \n",
    "    prediction1=tf.argmax(prediction,1)\n",
    "    a1 = sess.run(prediction1, feed_dict={x: a, keep_prob: 1.0})\n",
    "    \n",
    "    \n",
    "    pd.DataFrame({'imageid': Imageid, 'label': a1}).set_index('imageid').to_csv('sub4.csv')\n",
    "    \n",
    "        #print(sess.run(accuracy, feed_dict = {x: mnist.test.images, y : mnist.test.labels}))\n",
    "\n",
    "\n",
    "train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
